{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793dc9d7-2009-4f26-92ea-08daef468c76",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"color:lightgreen; font-size:30px\">**PG201 - Aprendizaje supervisado**</span>\n",
    "***\n",
    "<span style=\"color:gold; font-size:30px\">**Árboles de decisión (DT)**</span>\n",
    "***\n",
    "\n",
    "<span style=\"font-size:20px\"> **Autor: Kevin Alexander Gómez** </span>\n",
    "\n",
    "<span style=\"font-size:16px\"> **Contacto: kevinalexandr19@gmail.com | [Linkedin](https://www.linkedin.com/in/kevin-alexander-g%C3%B3mez-2b0263111/) | [Github](https://github.com/kevinalexandr19)** </span>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d96862-8d4b-4c2f-bec7-fef714eb7491",
   "metadata": {
    "tags": []
   },
   "source": [
    "Bienvenido al curso PG201 - Aprendizaje supervisado!!!\n",
    "\n",
    "Vamos a revisar las bases del <span style=\"color:gold\">aprendizaje supervisado</span> y su aplicación en Geología usando código en Python.\\\n",
    "Es necesario que tengas un conocimiento previo en programación con Python, álgebra lineal, estadística y geología.\n",
    "\n",
    "<span style=\"color:lightgreen\"> Este notebook es parte del proyecto [**Python para Geólogos**](https://github.com/kevinalexandr19/manual-python-geologia), y ha sido creado con la finalidad de facilitar el aprendizaje en Python para estudiantes y profesionales en el campo de la Geología. </span>\n",
    "\n",
    "En el siguiente índice, encontrarás los temas que componen este notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f752c8e-8060-4093-9ba9-0f2619d396d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Índice**\n",
    "***\n",
    "- [¿Qué es un árbol de decisión?](#parte-1)\n",
    "- [Árboles de decisión en Python](#parte-2)\n",
    "- [¿Podemos visualizar un árbol de decisión?](#parte-3)\n",
    "- [En conclusión...](#parte-4)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5a1bcf-40d1-4a18-97aa-b36732bb60b2",
   "metadata": {},
   "source": [
    "<a id=\"parte1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69113e1d-eba2-452e-997d-ed5fd26cd109",
   "metadata": {},
   "source": [
    "Antes de empezar tu camino en programación geológica...\\\n",
    "Recuerda que puedes ejecutar un bloque de código usando `Shift` + `Enter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7f1b4e-b233-46c1-9405-48fe2e626860",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9653a55-76fa-4b12-816e-c433f24879d0",
   "metadata": {},
   "source": [
    "Si por error haces doble clic sobre un bloque de texto (como el que estás leyendo ahora mismo), puedes arreglarlo usando también `Shift` + `Enter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d4794-9211-4db2-97b0-d553df067ffe",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef8a2e-3f26-411f-b624-5b7fc1c52ff2",
   "metadata": {},
   "source": [
    "<a id=\"parte-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14023483-8dfb-4dee-ba17-8c3d6eaa8d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgreen\">**¿Qué es un árbol de decisión?**</span>\n",
    "***\n",
    "\n",
    "De acuerdo con [IBM](https://www.ibm.com/es-es/topics/decision-trees#:~:text=Un%20%C3%A1rbol%20de%20decisi%C3%B3n%20es,nodos%20internos%20y%20nodos%20hoja.), un árbol de decisión se define como un <span style=\"color:gold\">**algoritmo de aprendizaje supervisado no paramétrico**</span>, empleado en la solución de problemas de clasificación y regresión.\n",
    "\n",
    "Este algoritmo destaca por su estructura jerárquica en forma de árbol, compuesta por un nodo raíz desde el cual se extienden las ramas hacia los nodos internos, culminando en los nodos hoja. La eficacia y simplicidad de su estructura lo hacen una herramienta valiosa tanto para interpretar cómo se toman las decisiones como para predecir nuevos datos.\n",
    "\n",
    "<img src=\"resources/decision_tree.png\" alt=\"Árbol de decisión\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec7f51-1d4d-490c-9749-b72f47277cab",
   "metadata": {},
   "source": [
    "El proceso de aprendizaje en los árboles de decisión implementa una estrategia de **divide y vencerás**, ejecutando una búsqueda codiciosa para determinar los mejores puntos de división dentro de un árbol. Este mecanismo de partición se aplica de manera recursiva, de arriba hacia abajo, hasta que se logra clasificar todos o la mayoría de los registros bajo categorías específicas.\n",
    "\n",
    "La capacidad del árbol para clasificar los datos en grupos homogéneos está directamente relacionada con su complejidad.\n",
    "\n",
    "Los árboles más compactos suelen producir nodos hoja homogéneos, donde los datos pertenecen a una única categoría.\\\n",
    "Sin embargo, conforme el árbol se expande, mantener esta homogeneidad se torna más complicado, resultando a menudo en la presencia de muy pocos datos dentro de un subárbol específico.\\\n",
    "Este fenómeno, conocido como fragmentación de datos, puede llevar a un **overfitting o sobreajuste** del modelo, donde este se ajusta excesivamente a los datos de entrenamiento, perdiendo capacidad de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42daf84-8039-40d4-b4da-62b11229a2f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<img src=\"resources/decision_tree_geology.png\" alt=\"Árbol de decisión geológico\" width=\"700\"/>\n",
    "\n",
    "Como resultado, los árboles de decisión muestran una tendencia a favorecer estructuras más compactas, lo cual resuena con el principio de parsimonia, comúnmente asociado a la Navaja de Occam. Este principio sostiene que \"no se deben multiplicar las entidades más allá de lo necesario\". En términos de árboles de decisión, esto implica que solo se debería incrementar la complejidad del modelo cuando es estrictamente necesario, ya que, generalmente, las explicaciones más sencillas resultan ser las más adecuadas.\n",
    "\n",
    "Para limitar la complejidad y prevenir el sobreajuste, se recurre a menudo a la técnica de **poda (pruning)**.\\\n",
    "La poda es un proceso que se encarga de eliminar aquellas ramas del árbol que se basan en atributos de importancia menor. Posteriormente, la eficacia del modelo ajustado puede ser evaluada a través de la validación cruzada.\n",
    "\n",
    "<span style=\"color:gold\">**¿Qué ocurriría si empleáramos múltiples árboles de decisión para realizar una predicción?**</span>\n",
    "\n",
    "Una manera de mejorar la precisión de los árboles de decisión es mediante la creación de un conjunto de estos a través del algoritmo de <span style=\"color:gold\">Random Forest</span>. Este enfoque permite obtener predicciones más exactas, especialmente cuando los árboles que componen el bosque no están correlacionados entre sí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e42f48-0201-43e0-b5e9-65b014e903d2",
   "metadata": {},
   "source": [
    "<a id=\"parte-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0ac0e-c888-4b50-8e3d-299d43c5c9a5",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**Árboles de decisión en Python**</span>\n",
    "***\n",
    "\n",
    "Empezaremos importando `pandas` para cargar el archivo `rocas.csv`.\\\n",
    "También importaremos algunas funciones de Sci-Kit Learn:\n",
    "> **Sci-Kit Learn** es una librería utilizada para construir algoritmos de machine learning, la referenciamos dentro de Python como `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce8d91-7aab-459a-91f8-f3c7adc503dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier      # Modelo de Árbol de Decisión para clasificación\n",
    "from sklearn.model_selection import train_test_split # Función para dividir los datos de entrenamiento y prueba\n",
    "from sklearn.metrics import accuracy_score           # Función para medir la precisión del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f98fa-d079-4510-b1b1-dce369db0d4f",
   "metadata": {},
   "source": [
    "Cargamos el archivo `rocas.csv` ubicado en la carpeta `files`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901bf9c9-6060-43d1-afbb-0ce400a3349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocas = pd.read_csv(\"files/rocas.csv\")\n",
    "rocas.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70576e7e-0feb-44b1-8c34-79cc5d3b2b68",
   "metadata": {},
   "source": [
    "Ahora, seleccionaremos las litologías de `Basalto` y `Riolita`:\n",
    "> Usaremos el método `isin`, filtrando aquellas filas que pertenezcan a las clases `basalt` o `rhyolite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de64518-0d05-49d1-a0a3-269ebf98bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rocas[rocas[\"Nombre\"].isin([\"basalt\", \"rhyolite\"])].reset_index(drop=True)\n",
    "data.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f28a7f-e559-456d-8ee2-6ec351278cc8",
   "metadata": {},
   "source": [
    "Crearemos una nueva columna llamada `target` y asignaremos un valor numérico a cada tipo de litología:\n",
    "> Los valores serán 0 para `basalt` y 1 para `rhyolite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9471cc1-da90-452f-809f-32093c0cc3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna llamada target de valor 0\n",
    "data[\"target\"] = 0   \n",
    "\n",
    "# Los datos de riolito en esta columna van a valer 1\n",
    "data.loc[data[data[\"Nombre\"] == \"rhyolite\"].index, \"target\"] = 1\n",
    "\n",
    "# Mostramos la tabla\n",
    "data.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf022f1-cab0-4f24-be2c-b4ca2ca0a85f",
   "metadata": {},
   "source": [
    "Luego de hacer la transformación de datos, separamos las columnas de la siguiente forma:\n",
    "- `X` : contiene la información numérica de concentraciones geoquímicas, usada para entrenar y probar el modelo.\n",
    "- `y` : contiene la información de la columna `target`, la variable a predecir.\n",
    "\n",
    "Usaremos el atributo `values` del DataFrame para convertir la información en arreglos de Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c3694-4815-4a04-b3f4-e0991f708129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:-1]   # Columnas de features\n",
    "y = data[\"target\"]       # Columna objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa2665-f3e4-4da9-86b0-6beb25974119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los features del modelo\n",
    "print(\"Features:\")\n",
    "print(\"----------\")\n",
    "print(X.values)\n",
    "print(\"\")\n",
    "\n",
    "# Mostramos el target del modelo\n",
    "print(\"Target:\")\n",
    "print(\"----------\")\n",
    "print(y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579e5db-5f2b-443c-b6cf-d09fcb3ff6dc",
   "metadata": {},
   "source": [
    "Una vez separado los datos, procedemos a separar la data de entrenamiento y de prueba usando la función `train_test_split`:\n",
    "> El parámetro `test_size=0.25` representa la fracción de la data que será asignada al conjunto de prueba.\\\n",
    "> También asignaremos un valor a `random_state` para que el resultado sea reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aff931-d40a-4cd1-b5d7-c1f21f835a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Tamaño de X_train: {X_train.shape}\")\n",
    "print(f\"Tamaño de X_test: {X_test.shape}\")\n",
    "print(f\"Tamaño de y_train: {y_train.shape}\")\n",
    "print(f\"Tamaño de y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb241b-c865-4880-88d6-f735a7f2d381",
   "metadata": {},
   "source": [
    "Ahora, crearemos el modelo y lo entrenaremos usando la data de entrenamiento:\n",
    "> El parámetro `criterion=\"gini\"` hace referencia a la impureza de Gini que establece el criterio de división de los nodos y `max_depth=2` establece la profundidad del árbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74c350-4040-4545-b78a-533dcb69240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e5208-28b6-459a-a1d4-b4db69db6c6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "> <span style=\"color:gold\">**¿Qué es la impureza de Gini?**</span>\n",
    ">\n",
    "> La impureza de Gini es un criterio utilizado para evaluar la calidad de una división en los nodos de un árbol de decisión dentro del contexto de los **árboles de clasificación y regresión (CART)**, un modelo introducido por Leo Breiman.\n",
    "> \n",
    "> Este índice mide qué tan a menudo un elemento seleccionado al azar sería identificado incorrectamente si se le etiquetase de acuerdo con la distribución de etiquetas en el conjunto. En otras palabras, evalúa la probabilidad de que un atributo sea clasificado erróneamente si se escoge al azar según la distribución observada en el subconjunto.\n",
    "> \n",
    "> Un valor de impureza de Gini de 0 indica la pureza perfecta, es decir, todos los casos en el nodo pertenecen a una sola clase, mientras que valores más altos indican mayor mezcla de clases dentro del nodo. En la práctica, al construir un árbol de decisión CART, el objetivo es minimizar la impureza de Gini al elegir el mejor atributo para dividir los datos en cada paso, buscando aquellos puntos de división que resulten en subconjuntos lo más homogéneos posible respecto a la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fa4e2-e6c0-4b70-afb1-b761ccfb7217",
   "metadata": {},
   "source": [
    "Procederemos ahora a entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85eab57-2a95-4455-8f82-38288eb47a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo de árbol de decisión\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3925b-2abe-4f82-a41a-3532bfe9c83d",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, evaluaremos su precisión usando la función `accuracy_score`:\n",
    "> La **precisión (accuracy)** representa la cantidad de predicciones que fueron correctas.\\\n",
    "> El parámetro `y_true` representa la data que se busca obtener y `y_pred` es la predicción realizada por el modelo.\\\n",
    "> Para predecir valores con el modelo, tenemos que usar el método `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1158aa9-8132-444e-9deb-2fa02ec4dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train) # Predicción del modelo con X_train\n",
    "y_test_pred = model.predict(X_test)   # Predicción del modelo con X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a4f5a-b871-47f7-a8b0-dd4e2274468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy Score - Entrenamiento: {accuracy_score(y_true=y_train, y_pred=y_train_pred):.1%}\")\n",
    "print(f\"Accuracy Score - Prueba: {accuracy_score(y_true=y_test, y_pred=y_test_pred):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a182c8f0-c00e-42b0-bb60-21d4e559b972",
   "metadata": {},
   "source": [
    "El modelo de arbol de decisión ha obtenido una alta precisión (más del 99%) para discriminar muestras de basalto y riolita.\\\n",
    "Al tener precisión alta tanto en el entrenamiento como en la prueba, podemos concluir que no existe sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe55f1-e47a-4612-89c1-efbc55f397d9",
   "metadata": {},
   "source": [
    "<a id=\"parte-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279c233-54e7-4e0b-9d54-ac667fb55717",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">**¿Podemos visualizar un árbol de decisión?**</span>\n",
    "***\n",
    "\n",
    "La respuesta es sí, y para esto, utilizaremos las funciones `export_text` y `plot_tree` del módulo `sklearn.tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80136f70-0888-499e-9533-0ba0dd01226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_text, plot_tree   # Funciones para graficar el árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a216f14-843b-485d-8dc9-32c7a1041695",
   "metadata": {},
   "source": [
    "Crearemos una variable llamada `x_cols` para almacenar los nombres de las columnas de X:\n",
    "> Seleccionamos las columnas a partir de la segunda posición (1) hasta antes del último (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcbbd7-4a9a-43ea-ad53-259d25339d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenamos los nombres de las columnas de X\n",
    "x_cols = list(X.columns)\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fb186-92a0-45ef-90a0-df9d95266c79",
   "metadata": {},
   "source": [
    "Exportamos los parámetros del árbol de decisión usando la función `export_text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1b855-198b-4ff5-8b32-b6250fc17c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = export_text(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2374f2-58ee-4b41-b147-177b86f5740e",
   "metadata": {},
   "source": [
    "Ahora, procedemos a graficar el árbol de decisión usando la función `plot_tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ee882-64b1-4d69-9056-43e90a87213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_tree(model, feature_names=x_cols, class_names=[\"Basalto\", \"Riolita\"], filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4606c13-42bb-4434-a58e-60e55ea30075",
   "metadata": {},
   "source": [
    "Por último, observaremos la importancia de cada columna usando el atributo `feature_importances_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016613f-722c-4e7a-ad15-cf097e4cbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importancia de atributos\")\n",
    "for col, imp in zip(x_cols, model.feature_importances_):\n",
    "    print(f\"{col}: {imp:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1fc0fe-4285-4342-9155-a8e9f0ab0c35",
   "metadata": {},
   "source": [
    "La **importancia de atributos** nos ayuda a determinar qué variables son las más importantes para entrenar el modelo.\\\n",
    "Observamos que la columna `SiO2` tiene una importancia muy alta (más del 99%) comparada con el resto de columnas.\\\n",
    "Algunas columnas son irrelevantes para el entrenamiento del modelo.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc7569-f76a-4dc3-9032-19ce54bb28a0",
   "metadata": {},
   "source": [
    "<a id=\"parte-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a14e0d-f144-49f8-bd95-7979897a5634",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgreen\">**En conclusión...**</span>\n",
    "***\n",
    "\n",
    "- La naturaleza jerárquica de un árbol de decisión facilita ver qué atributos son los más importantes, por lo que son **más fáciles de interpretar** que otros modelos de aprendizaje automático.\n",
    "- Los árboles de decisión tienen una serie de características que los hacen más flexibles que otros clasificadores.\n",
    "- Los árboles de decisión se pueden aprovechar para tareas de clasificación y regresión, lo que los hace más flexibles que otros algoritmos.\n",
    "- Los árboles de decisión complejos tienden a sobreajustarse y no se generalizan bien a los nuevos datos. Este escenario se puede evitar mediante el proceso de poda (pruning).\n",
    "- Pequeñas variaciones dentro de los datos pueden producir un árbol de decisión muy diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbccfa-e0aa-4cfa-ad65-83515bad4723",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
